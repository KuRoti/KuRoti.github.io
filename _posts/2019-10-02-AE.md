---
title: "AutoEncoder"
date: 2019-10-02 12:26:28 -0400
categories: deleopment
tags:DL
layout: post
---

# 오토인코더

[TOC]

GAN이랑 같이 맞물려서 같이 쓰는게 주요 흐름이다. 

위키피디아에서 뽑은 키워드는 5개. 

Unspervised Learing, representation learning, Dimensionality reduction, Generative model learning.

Dimensionality reduction이 가장 주요한 오토인코더의 역할.

Nonlinear dimensionality reduction을 위키에 쳐보면 오토인코더가 주요하게 나옴. 

키워드를 요약해보면 Nonlinear Dimensionality reduction

<img src="C:\Users\mhd07\Desktop\KU\blog\post\오토인코더 분류.png" alt="오토인코더 분류" style="zoom:50%;" />

GAN을 만드신 분의 정리에 따르면 Maximum likelihood와 관련이 있음.

최종적으로 오토인코더의 키워드를 요약하면

1. Unsupervides learning
2. Manifold learning
3. Generative model learning
4. ML density estimation

Loss function은 negative ML로 해석되기에 Loss를 최소화하는 Maximum likelihood method와 연결됨.

인코더부분과 디코더 부분으로 나누는데, 보통 오토인코더는 input의 차원이 더 크기에 차원을 초정하는 manifold learning이 사용됨. 

Decoder 파트는 이미지를 생성하는 Generative model이 사용됨.

Manifold Learning이 오토인코더를 이해하기 위해 중요한 개념이기 때문에 챕터 하나에서 따로 설명하신다고 합니다.

## Revisit Deep Neural Networks

이 챕터의 목적은 Deep Neural net사용을 해석할 때 Maximum likelihood estimation을 쓴다는 것을 이해하는 것이라고 함.

### Machine learning problem

머신러닝의 요소는 데이터와 데이터에서 뽑고 싶은 추상화된 정보. 그래서 데이터를 열심히 수집하고 모델을 선정한다. 모델이 머신러닝에서 학습을 하는 대상이기에 모델을 정의해줘야함. 모델을 정해놓고 모델을 결정짓는 파라미터를 추정하는 단계가 학습 과정. 파라미터를 θ라고 하면 θ에 대해서 입력 데이터의 출력값과 실제 출력되어야 할 값의 차이를 Loss function으로 정의하고, Loss function이 최소가 되는 θ를 구하는 것이 training. Training이 끝나면 Training이 일어나지 않은 데이터 set에 대해 값을 도출해보고 테스트를 한다.

Loss function은 back propagation의 제약조건을 만족해야 하기에 아무 함수나 사용할 수 없음. mean squere 랑 cross entropy가 딥러닝 프레임워크에서 사용되는 거의 유일한 Loss function. Deep Neural Net의 Loss function의 가정 둘.

1. ##### 트레이닝 전체  loss함수는 각 training sample들의 loss 값의 합이다.

2. ##### Loss function을 구성할 때 네트워크의 출력값과 정답만으로 구성한다.

네트워크의 종단에서만 값을 사용하고, 네트워크 중간에 있는 값은 사용하지 않는다는 뜻.

지금 대부분의 방식이 Gradient Descent 방법. 

`θ* = argmin L(f_θ(x),y)` 라는 θ에 대해

Gradient Descent는 한 번에 답을 구할 수 있는 것이 아닌 Iterative Method로서 점차 답을 구해가는 방법이다. Iterative Method는 θ 업데이트를 위한 방법과 θ 업데이트를 멈출 방법의 두 가지 조건을 필요로 한다. Gradient Descent는 θ가 줄어드는 방향으로 가고, 왼쪽으로 가도 오른쪽으로 가도 θ가 변하지 않으면 중지.

θ의 dimension이 너무 커서 문제가 발생.

Taylor Expansion > L(θ+Δθ)을 전개하면 나오는 항들 중 1차 계수 항까지만 사용한다. 1계 미분 항까지만 써서 approximation을 할 것이고

이 두 항만 보게 되면 L(θ+Δθ)를 L(θ) + L' ·Δθ 로  가정할 수 있다.

Δθ = -ηL'로 두어서 정의해버리면
$$
\Delta L=-\eta\|\nabla L\|^2 <0
$$
가 된다. Learning rate $$ \eta $$가 양수라는 조건 아래에서 로스가 줄어든다는 것이 보장된다는 결론을 얻을 수 있다. 위의 근사 조건을 위해서는 Learning rate가 작아야 하기에 정확도를 위해서 작은 Learning rate를 쓴다.

Gradient Descent. 샘플 수가 너무 많아서 전체 sample들에 대해 gradient를 하기 힘들다. 그래서 Batch size의 sample들의 평균에다가 gradient를 구하고 이게 전체 sample들의 gradient를 구했을 때 이를 사용해도 똑같으리라 기대하는것.

딥 뉴럴넷이기 때문에 각 Layer별 weight, Loss 함수들이 존재. 너무 많은 양이라 파라미터들을 모두 계산하기 힘들었는데, Backpropagation이 이를 해결함. 

딥 뉴럴넷을 학습시키는 것과 Maximum likelihood estimation을 하는 것이 같은 이야기라는 것을 이해시키기 위해 하나하나 설명중. 

#### BackPropagation Algorithm

1. output layer에서 error signal을 구한다.
2. 각 Layer별로 error signal을 구한다.
3. Bias에 대한 미분값 구하기.
4. Weight에 대한 미분값 구하기.



### Loss function viewpoints

#### Back-Propagation

Back propagation의 가정을 만족하는 두가지가 Mean Square Error와 Cross Entropy

두 Loss 함수의 장단을 알고 선택해야함. 예를 들어 Maximum liklihood에서는 continuous한 value면 Mean Square를, 이산적이면 Cross Entropy를 사용하는 것이 나음.

Back Propagation 관점에서 

Mean square error를 썼다는 것은 ???

초기값 차이가 Loss 함수 감소율에 영향을 크게 미침. Back Propagation 관점에서 보면 수식 자체에 activation function의 미분값이 들어감. 

Bad Case) Sigmoid의 미분값이 0에 가까운 값 지점이 걸리면 Gradient 값이 0에 가까워져서 학습률이 낮아짐. 

반면 시작부터 Sigmoid의 미분값이 큰 지점에서 시작하면 처음부터 학습이 잘됨. 

Gradient vanishing Problem > 미분값이 뒤로 갈수록 작아지니까...자꾸 Vanish. 그래서 최근에 잘 쓰이는 함수가 ReLu함수인데 이 Relu는 범위내에서 미분값이 1이라 뒤로 가도 사라지지 않음. 이런 관점에서도 해석할 수 있다!



Cross Entropy는 error signal에 $$ \sigma'(z) $$ 항이 없어서 초기 포인트의 activation function 미분값이 반영되지 않아 이런 문제가 발생하지 않음.  그래서 Back Propagation은 Cross Entropy가 한 레이어는 쉬고 가니까 조금 더 유리하다. 

#### Maximum likelihood

Maximum likelihood는 네트워크의 출력값이 있을 때 우리가 원하는 정답이 나올 확률이 높길 바란다. 즉, 데이터 $$x$$, 출력 $$y$$, 모델 $$f_\theta$$에 대해 Likelihood 값, $$ p(y|f_{\theta}(x)) $$ 가 최대가 되길 원한다.

네트워크의 출력값과 정답이 가까워졌으면 좋겠음. 

$$\theta^* =argmin_\theta[-log(p(y|f_\theta(x)))]$$ , 주어진 데이터를 가장 잘 설명하는 모델을 찾는다.

여기서는 확률 분포를 찾은 것이기 때문에 고정된 입력에 대해 다른 출력을 내놓을 수 있다. 즉, Sampling을 할 수 있다는 것이 가장 큰 장점이다. 여기서 Maximum likelihood estimation을 하겠다고 하면 평균을 출력하면 고정적인 출력을 내놓을 수도 있음.

$$p(y|f_\theta(x))$$는 iid_condition을 만족한다.

Gaussian distribution이냐 Bernoulli distribution이냐를 정해야됨. 다른 모델도 있으나 수학적으로 좀 어려움.

Gaussian을 쓰면 결국 Mean Squared Error로 유도됨.

Bernoulli Distribution으로 보면 결국 Cross-entropy로 유도됨.

이런 면에서 보통 연속적인 값에선 Mean Squared Error를 쓰고, Classification같은 이산적인 값을 가질 땐 Cross Entropy를 쓰는게 유리하다고 해석할 수 있다. (항상 그렇다는 것은 아님)

### Maximum likelihood for autoencoders

|                          | Autoencoder             | Variational Autoencoder |
| ------------------------ | ----------------------- | ----------------------- |
| Probability distribution | $$p(x|x)$$              | $$p(x)$$                |
| Gaussian distribution    | Mean Squared Error Loss | Mean Squared Error Loss |
| Categorical distribution | Cross-Entropy Loss      | Cross-Entropy Loss      |

오토인코더는 네트워크 입력이 $$x$$, 출력도 입력과 똑같길 바람. 그래서 $$p(x|x)$$를 추정하길 바라는 것이 Autoencoder.  

## Manifold Learning

고차원 데이터를 사용함. Manifold Learning은 데이터 차원을 축소하는 것.

고차원 데이터는 데이터의 차원 그 자체임. 

각 training sample들을 잘 아우르는 어떤 subspace가 manifold이고, 이 manifold를 잘 찾아 projection시키면 차원을 축소할 수 있다고 생각함.

원래 데이터의 정보를 잘 유지하면서 차원을 축소하는 것이 Manifold Learning의 목표. 

### Four objectives

1. Data compression

   - 데이터를 줄이고 복원했을 때 잘 되면 좋은 것. 이미지 압축은 보통 JPEG를 쓰는데 오토인코더를 쓰면 더 압축률이 높다는 논문이 17년도에 나옴. 

     (Lossy Image Compression with compressive autoencoders, 17.03)

2. Data Visualization

   - 최대 4차원 이하여야 Visualization이 가능하기 때문에 차원을 압축할 필요가 있음. 
   - http://vision-explorer.reactive.ai/#/galaxy?_k=sewtkz -> Visualization 끝판왕 예시. Google의 Classification 결과를 Visualization한 것.

3. Curse of dimensionality

   - 데이터의 차원이 커지면 공간의 전체 크기가 지수적으로 증가하기에 동일한 개수의 데이터가 있어도 데이터의 밀도가 매우 희박해짐.
   - 따라서 차원이 커지면 모델 추정에 필요한 샘플 수가 기하급수적으로 증가함.
   - Manifold Hypothesis : 고차원에서의 데이터 밀도가 낮아도 이를 포함하는 저차원의 Manifold가 있다는 추정.  

4. Discovering most important features

   - 압축을 잘 했으면 feature를 잘 찾았다는 뜻. mnist같은 유명한 데이터셋에 대해 압축을 해보니 thinkness나 rotation같은 특성들이 manifold 2차원 평면에서 나타났음.
   - Reasonalble distance metric - 원래 고차원 공간에서의 샘플간의 거리와 Manifold에서의 sample간의 거리가 차이남. 고차원 거리가 가까우면 데이터 자체가 가깝다는 뜻, manifold에서 가까우면 feature가 가깝다는 뜻. 
     두 sample의 의미적 중간 sample을 생성하고 싶다면, 원래 차원에서 중간지점을 뽑는 것이 아님. Manifold상에서 중점을 잡아야 중간 feature를 기대할 수 있음.
   - Disentangled/Entangled Manifold. Mnist의 예시에서 학습이 잘된 manifold는 dominant feature가 숫자이기에, 다른 숫자값을 가진 데이터는 manifold안에서 확실히 구별되길 희망함. Disentangled Manifold에서는 서로 다른 dominat feature들은 육안으로 구별되어 보임.

### Dimension reduction

1. Linear

   1. Principal Component Analysis (PCA)

      projection을 통해 차원을 감소시킴. 

      $$ h= f_\theta(x)=W(x-\mu) with\theta=\{{W,\mu}\}$$ 

      오토인코더는 PCA를 포함하는 방법론임. 

      PCA는 Linear한 방식이기 때문에 꼬여있는 값에 대해서는 Entangled Manifold가 나옴.

   2. Linear Discriminant Analysis (LDA)

2. Non-Linear

   1. Autoencoders (AE)
   2. t-distributed stochastic neighbor embedding (t-SNE)
   3. Isomap
      $$\epsilon$$-isomap이랑 k-isomap으로 나뉨. 각 샘플별로 근방에 있는 것을 구하고 거기서 가까운 애들은 manifold 내에서도 가까울 것이라고 가정함.
   4. Locally-linear embatting(LLE)

   -> 오토인코더를 제외한 기존의 방법론들은 Neighborhood based training임. 하지만 고차원 데이터 간의 Euclidean distance는 유의미한 feature의 거리 개념이 아닐 확률이 높음. 그래서 Autoencoder는 고차원의 많은 데이터에서 유리함. 

### Density estimation



## Autoencoder

### Autoencoder (AE)

2000년 초반에 나온 구조. 입력과 출력이 같게되고, 가운데에서는 차원이 줄어드는 구조. 처음에는 가운데 차원이 늘어나는 구조를 열심히 연구해서 sparse autoencoder를 썼음. 

가운데 작은 Layer는 Hidden representation, Latent Variable라고도 부름.

Loss함수가 $$L(x,y)$$임.

Demensionality reduction이 사실 supervised Learning으로 풀 수가 없는 문제임. 그래서 어쩔수없이 unsupervised learning으로 풀어야 함. 그런데 우리가 X,Y를 아니까 이 문제를 supervised learning으로 바꾸어서 풀 수 있게 되었다는 점이 AE가 처음 나왔을 때 각광받은 이유.

학습이 끝나면 Encoder와 Decoder를 분리해서 씀. 

Encoder는 최소한 학습 데이터에 대해서는 압축을 잘 시킨다.

Decoder는 최소한 Training DB의 데이터는 만들어낼 수가 있음. Minimum 성능 보장이 없는 GAN에 비한 장점. 대신 단점-> Training DB에 있는 것만 만들다 보니까 새로운 걸 만들고 싶어도 DB에 있는 것과 비슷하게 만들어짐.

General Autoencoder에서 hidden layer를 Activation함수 없이 쓰는게 Linear Autoencoder.

linear autoencoder를 구성하고 Mean-squeared error 로스를 쓰면 PCA랑 똑같은 Manifold를 형성한다는 논문이 나와있음.

Autoencoder는 굉장히 사용하기 편한 편. 오토인코더가 나온 시기랑 최근과 같은 딥러닝이 유행한 시기가 많이 차이남. 그래서 처음엔 오토인코더로 pretraining이랑 demension reduction 두 목적으로 쓰임. 오토인코더를 쌓아가면서 pretraining하던 거를 stacking autoencoder라고 불렀음.

### Denoising AE (DAE)

입력을 그냥 입력을 주는 것이 아니라 노이즈를 추가해서 넣음. 노이즈가 추가되어 들어간 x'이 x가 되도록 학습시킴. 그래서 Denoising을 함.

이게 처음엔 manifold learning 차원에서 탄생했었음. 사람이 봤을 때 의미적으로 똑같다고 느낄 만한 노이즈를 추가하면 manifold상에서 같은 공간에 mapping되어야 한다는 idea에서 시작함. 

DAE 최초의 논문에서 AE의 weight들과 DAE의 weight를 visualize해서 비교해서 1st layer에서 얼마나 feature edge를 잘 detect하냐고 보여주었었음.

DAE의 variation으로 SDAE, Stacked DAE가 있음. 노이즈를 추가해서 pretraining의 효율이 높아짐. SDAE의 Generation performance도 SAE보다 훨씬 좋았음.

### Contractive AE (CAE)

CAE 나오기 전에 Stochastic Contractive AE가 나왔음.

$$L_{SCAE} =\Sigma_{X\in D}L(x,g(h(x)))+\lambda E_{q(\tilde{x}|x)}[\|h(x)-h(\tilde{x})\|^2]$$

 			=  reconstruction error + Stochastic Regularization

Deterministic하게 이를 개선한게 CAE임.

보통 CAE는 안쓰고 상식상 알아두는거임.

3대장이 AE, DAE, Variational AE이 세가지.

## Variational Autoencoders

제일 많이 쓰는 오토인코더의 방법. 일단 오토인코더로 압축 해보고, 고전적인 접근론들로도 해봄. 잘 압축이 되어있다면 고전적인 접근법으로도 가능. 

### Variational AE (VAE)

코드를 봤을 땐 AE랑 똑같이 보이나 논문의 흐름을 따라가보면 전혀 관계가 없음. AE는 VAE와 사실은 전혀 관계없음. AE는 manifold Learning이 목적이었음. unsupervised를 학습하기 위해 supervised로 바꿈. VAE는 Generating이 목표였고, 이를 위해 뒷 생성파트를 위해 앞 파트를 붙인것, 그래서 결과적으로 똑같이 보이는 것.

트레이닝 DB에 있는 데이터포인트의 $$p(x| g_\theta(z))=p_\theta(x|z)$$ 라는 generate 확률을 구함. Generate는 기존의 이미지를 만드는 것. 

$$z$$는 generating을 위한 변수.

ex) 사람의 얼굴을 만들 때 무작위로 만들기보다는 여자/남자 같은 parameter를 기반으로 만들면 좋다.

$$g_\theta()$$는 $$\theta$$로 매개되는 Deterministic function.

실제로 배워야 할 manifold가 복잡하다 하더라도, 우리의 모델은 딥 러닝 모델이기 때문에 한 레이어 정도가 적당한 latent space로 mapping 해줄것이다.  

Q) 왜 maximum likelihood estimation을 바로 쓰면 안되는가?

$$p(x)\approx\sum_ip(x|g_\theta(z_i))p(z_i)$$ likelihood값이 높길 바라는데, 가우시안 분포에서 평균에 해당하는 애가 작은 애로 선택되는데, mean squared error에서 나오는 애는 우리가 원하는 애가 아님.

저런 이유 때문에 바로 data에서 sampling을 했더니 잘 안나옴. 그래서 이상적인 sampling 함수를 생각해봄. 

$$p(z|x)\approx q_\phi(z|x)\sim z$$ 라는 함수를 가정함. $$z$$ smaple을 잘 만들어내는 이상적인 generate 함수를 만들고 싶다. 우리가 결국 구하고 싶었던 것은 $$p(x)$$였는데 새로운 개념들이 많이 나옴. $$p(z|x),q_\phi(z|x)$$ 요 두놈과의 관계. 

잘 요약하면 
$$
log(p(x))=\int log(p(x|z))q_\phi(z|x)dz-\int log(\frac{q_\phi(z|x)}{p(z|x)})q_\phi(z|x)dz
$$
라는 식이 나옴. 그래서 
$$
ELBO(\Phi)=\int log(\frac{p(x,z)}{q_\phi(z|x)})q_\phi(z|x)dz
$$
라는 $$ELBO(\Phi)$$함수를 얻을 수 있음. 

이 ELBO함수를 maximize하는 것이 가장 이상적임.

그래서 $x$를 입력으로 갖고 중간에 ~$z$ 개의 sample을 갖고 다시 $x$개의 출력을 갖는 뉴럴넷이 완성됨. 만들고보니 AE형태. 하다보니까 variationa Inference 방법을 쓰는 오토인코더 형태가 되었기에 variational AE라고 부르는것. 

$ELBO$를 조금만 더 자세히.

#### ELBO

= Reconstruction Error+Regularization

##### Reconstruction Error

어떤 분포를 쓰느냐에 따라 에러 종류가 달라짐.

##### Regularization

KL함수임. KL을 minimize해야하는데 $q_\phi,p$를 minimize하는 것을 원함.

EElbo term을

$\phi$에 대해서 macimize-> 이상적인 샘플링 함수

$\theta$에 대해서 찾으면 maximum likelihood 관점에서 네트워크 parameter를 찾는 것.

이 둘을 동시에 함.

KL term부터 계산하면 prior는 normal distribution으로 함.

$q_\phi$는 가우시안으로 가정함. $\mu, \sigma$를 찾아야 한다.

가우시안 분포 두개간의 KL divergence에 대한 공식은 이미 나와있음. 그래서 KL term을 계산하면 closed form으로 KL term이 계산이 됨. 코드에도 이 내용은 그대로 나옴. 

Mnist VAE코드를 보면 encoder에서는 $\mu,\sigma$를 찾음. 그래서 이 둘을 찾았으니 KL_divergence를 찾을 수 있음.

reconstruction error구하기

원래 적분해야하지만 몬테카를로 기법으로 가정해버림. $L$개를 뽑아서 디코더를 태워버리고 파라미터를 찾고 likelihood값을 찾아서 함.

reconstruction에러를 구하는데 $L$개를 sampling하는 과정이 랜덤으로 들어가는게 문제. 랜덤한 요소때문에 back-propagation이 안됨ㅠㅠ.

논문에서는 가우시안 분포를 따른다고 할 때 확률적 특성은 똑같지만 랜덤 노드가 안생기게 하는 reparameterization trick을 쓴다. 

이거는 사실 안써도 됨. Tensorflow의 api 내부에 자동으로 reparameterization trick이 있어서 코드에 안넣어도 자동으로 지원해준다.

Generator랑 decoder의 확률분포를 각각 정하고 해야하니 베르누이를 쓰냐 가우시안를 쓰냐인게 결정사항인데 베르누이를 보통 쓴다. 이미지에서는 가우시안으로도 쓰기도 함. 베르누이를 쓰면 Cross entropy, 가우시안을 쓰면 MSE가 됨. 

#### structure 요약.

encoder는 $q_\phi$가우시안, 중간에는 reparameterize를 씀. encoder는 가우시안을 쓰는 이유는 다른 확률분포 함수에 대해 계산이 엄청 어려움. 그래서 가우시안->reparameterize 부분은 여러 모델들이 다 동일함.

그래서 decoder 부분에 베르누이를 쓰느냐 가우시안을 쓰느냐에 따라서 전체 모델이 Cross entropy와 MSE의 차이가 발생함.

코드 과정에서 AE랑 VAE의 차이는 KL term밖에 없음. sampling이 항상 같은 지점에서 한다고 하면 KL term만 다름. 

오토인코더의 목적은 데이터 압축, VAE의 목적은 데이터 생성. 학습이 끝나면 AE는 앞부분만 쓰면 됨. generation 관점에서 보면 decoder를 쓸때 의미있는 range가 나오는 z의 차원을 모름. 

reconstruction은 똑같이 하는데 z에 대해서 다름. AE는 z들이 생성할 때마다 다르게 분포됨.

VAE는 이상적인 샘플링 함수가 prior랑 똑같았고 걔를 잘설명하게 학습했기 때문에 z가 normal하게 나옴. 

### Conditional VAE (CVAE)

VAE를 다시 보면 입력이 있고 히든 레이어 두개 다음에 $\mu,\sigma$를 찾고 z 거쳐서 히든 레이어 두개를 거치고 출력으로 가는데 CVAE는 다름.

VAE는 공간 자체를 학습할 때 label 정보를 안넣고 unsupervised 방법으로 함.

label 정보를 알면 쓰면 되는데 이 label 정보를 쓸 수 있도록 개편한 것이 CVAE. encoding 과정과 decoding 과정의 첫 번째 레이어에 각각 label 정보를 넣음. 조건부 확률 정보가 살짝 바뀌고 elbo 식은 그대로. 

label 정보를 다 알고있다면 그렇게 하면 되는데 semi-supervised도 지원함. 알고있을때는 CVAE를, 모르는 애는 모르는 애에 대해 y를 추정하는 레이어를 별도로 두고 추정한 값을 넣어줌. 

비슷한 다른 semi supervised방식은 VAE를 먼저 학습시킨 다음에 이 위에 label 추정하는 네트워크를 붙여서 다시 학습함. 

CVAE와 VAE를 비교하면 확실히 CVAE가 학습이 빨리 일어남. 복원이 빨리되고 denoise도 빨리됨.

이걸 잘 활용하면 feature들에 대해서 dominant한 feature들을 자동으로 학습할 수 있게할 수 있음.

z값을 고정하고 label만 바꾸면 필체에 대한 style을 고정한 채로 다른 숫자를 생성할 수 있음. 그래서 이 기술로 손글씨 font 만들어주는 회사도 있었음.

semi-supervised 방식으로도 학습 가능한데 mnist sample 5만개 중에 100개만 label 줬는데 분류 성능이 95%가 나옴. 

### Adversarial AE (AAE)

VAE를 보면 다 디코더를 바꾸는 것이었음. 가우시안 이외의 KL 텀 계산이 어렵다는 한계가 있었는데 이거에 도전한게 AAE.

KL divergence는 계산하지 않아도 되고 sampling은 가능한 함수로 써보자는 것. 원래 KL term이 확률분포 두개를 같게 만드는 목적이었는데 $q_\phi, p$가 맞아지게 만드는거였음. 근데 이게 GAN 방법론이었음. 그래서 KL대신에 GAN loss를 쓴다.

#### GAN

generative adversarial network.

VAE는 density를 정하고 가는거고, GAN은 density를 정하지 않고 가는 거다. 이게 장점으로 작용함.

generator 거치면 뭔가 나옴. 이건 가짜 샘플임. true distribution에서 진짜 샘플을 가져와서 discriminator 네트워크에 가짜 샘플과 진짜 샘플을 넣어주고 구분하도록 시킴. 그래서 Discriminator는 구별능력을 학습하고, generator는 fake sample 생성능력을 학습함. Generator 입장에서는 discrimizate된 generated image의 값이 1이길 바람. 서로 적대적인 입장이라서 'adversarial' network임.

얘는 그래서 이 discriminator랑 generator의 균형을 맞추면서 같이 키우는 것이 어려워서 VAE가 더 쓰기 쉽다고 함.

 그래서 수식으로 봤을 때, $G(z)\sim p_{data}(x)$로 만드는 것이 목적임. 



이런 GAN을 참고해서 오토인코더를 다시 디자인함. discriminator를 두고 학습을 시켜서 z에서 오는  q(z)랑 실제 함수에서 오는 p(z)를 discriminate하게 만들고 그걸 KL 대신에 쓴다.

VAE는 목적이 같으니 한번에 돌리면 되는데 GAN은 적대적인 두 네트워크를 번갈아가면서 한번에 하나 멈추고 다른거 학습하기를 반복해야함. 그래서 AAE도 세 단계를 반복하며 천천히 학습함.

AAE는 좀더 prior에 가깝게 나오고, VAE는 sample의 분포에 가깝게 나옴. 

만약 prior를 10개의 가우시안 분포의 mixture로 한다고 하면 AAE는 그 자체 그대로 이쁘게 나옴. 그런데 VAE는 형태가 나오긴 하는데 좀 컹스.

그런데 이 AAE 모양을 보니 날개가 10개 모양이라서 하나별로 하나의 label을 의미하게 설계해보고 싶음.

그래서 discriminator에다가 label 정보를 넣어줌. 그럼 가짜 분포가 진짜 분포를 따라가도록 할 수 있고 한 날개에 대해 하나의 label들을 의미하도록 만들 수 있음. 각 gaussian 분포에서 동일 위치에 있는 것들은 같은 스타일을 갖도록 가능.

AAE의 최대 장점은 manifold를 우리가 원하는 방법으로 만들 수 있다는 점. swiss roll모양이나 10 gaussian mixture같은 모양으로 각각 가능. 

실제 코드를 보면 loss가 세개나 존재. 각 loss를 learning rate로 학습시킴. gan 학습이 그냥 하면 잘 안되니까 discriminator는 천천히, generator는 빠르게 학습하는 것이 테크닉임. 

AAE에서 KL조건뿐 아니라 sampling도 안되도 괜찮다는 AVB라는 논문이 나오기도 함. 

## Applications

오토인코더가 Dimension reduction을 진짜 잘 함. 잘만 맞춰주면 feature를 잘 뽑는다. VAE 데모 morphing faces를 보면 z demention을 29로 압축함. 흑백 얼굴 영상으로 쭉 학습을 함. 진짜 이미지 처럼 보이기보단 블러처리 된 것처럼 보임.  픽셀 로스의 합이기 때문. training DB의 평균 이미지가 나오는 느낌이 강함. 각 element가 뭔지는 잘 모름. 하지만 계속 블러하게 나옴. 

CVAE는 mnist의 feature를 12 dimension으로 줄여서 막 할 수 있음. 

GAN은 훨씬 자연스러운 이미지가 나옴. 

celeba DB라고 유명인 얼굴의 DB가 있음. 블러리한 이유를 제거한 VAE-123 모델도 있고 BEGAN 모델도 있음. 

quick draw라는 사이트 있음.  이게 재미있어서 사람들이 많이 쓰니까 스케치 데이터가 많아짐. 그래서 VAE를 이걸로 학습시킴. sketch RNN VAE

​	Text

​		semantic Hashing

​		Dynamic Auto-Encoders for Semantic lndexing

​	Image

​	Sound

### Retrieval

### Generation

### Regression

### GAN+VAE

#### comparison between VAE vs GAN

VAE는 두 모델이 같은 목적으로 감. GAN은 반대로 감.

이미지는 VAE는 블러리, GAN은 샤프. 

VAE는 explicit하니까 확률 분포를 가정하고 시작하기 때문에 여러 데이터 stream이 있으면 그 흐름을 모두 커버하는 flow를 만듬.

GAN은 generator가 만든 애들이 진짜같기만 하면 되니까 GAN은 여러 stream(mode라고 함) 중에 하나만 익혀도 학습이 끝난다. 그래서 다양성이 사라지는 단점이 있음.

그래서 GAN이랑 VAE를 같이 쓰는것이 추세임

##### EBGAN

discriminator를 AE로 넣고 진짜면 reconstruction error를 높게, 가짜면 낮게라고 하면 GAN보다 잘됨. 이게 EBGAN. 

##### stack GAN

우와 natural language를 분석해서 image 생성 > stackGAN. VAE로 feature를 '잘' 뽑아서 하는거... GAN을 두번 써서 stack GAN이라고 하는 것.

##### 3DGAN

2D 이미지를 보고 2D 이미지에 맞는 3D이미지 생성. 2D를 잘 뽑기 위한걸 AE로 잘 학습하고 벡터를 뽑음. 이걸 generator로 뽑은다음에 여러 3D 모델 만들고 이 모델들 중에 discriminator로 뽑는거임.

##### SEGAN

음성을 denoising한다. 배경 노이즈를 제거하는 능력이 뛰어남. 

##### papers in CVPR2017

이미지를 주고 나이를 바꿈. image랑 나이 정보가 있는 DB가 있음. 하지만 같은 사람에 대해 나이대별로 있는 게 아님. CAE에서 차원을 잘 맞춰서 manifold를 이쁘게 매칭해서 한거.  학습된 공간을 uniform distribution으로 설정해서 prior를 그걸로 학습함. label에 나이 정보 넣어주고 decoding을 한 다음에 reconstruction error를 봄.  

다른건데 색상 팔레트만 바꿔서 채색을 다르게 해버리는 논문. PaletteNet. 
